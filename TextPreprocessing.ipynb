{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0014dd8",
   "metadata": {
    "id": "a0014dd8"
   },
   "source": [
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b92c4f1",
   "metadata": {
    "id": "2b92c4f1"
   },
   "outputs": [],
   "source": [
    "# If running in a fresh environment, uncomment the line below:\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a95322b",
   "metadata": {
    "id": "1a95322b"
   },
   "source": [
    "## Importing Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6ef57",
   "metadata": {
    "id": "04e6ef57"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load text_dataset (adjust path as needed)\n",
    "text_df = pd.read_excel('text_data.xlsx')\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8639f6",
   "metadata": {
    "id": "cf8639f6"
   },
   "source": [
    "## Text Processing Pipeline Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd346900",
   "metadata": {
    "id": "cd346900"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b145665",
   "metadata": {
    "id": "0b145665"
   },
   "source": [
    "# Lowercasing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b3dfb5",
   "metadata": {
    "id": "e8b3dfb5"
   },
   "outputs": [],
   "source": [
    "# Preview a sample article before lowercasing\n",
    "sample_article = text_df['Article'][0]\n",
    "print(sample_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7fd378",
   "metadata": {
    "id": "ed7fd378"
   },
   "outputs": [],
   "source": [
    "# Convert sample article to lowercase\n",
    "sample_article = sample_article.lower()\n",
    "print(sample_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be5de5f",
   "metadata": {
    "id": "2be5de5f"
   },
   "outputs": [],
   "source": [
    "# Apply lowercasing to the entire text_dataset\n",
    "text_df['Article'] = text_df['Article'].str.lower()\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a4b98b",
   "metadata": {
    "id": "30a4b98b"
   },
   "source": [
    "# Special Character Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f69eb9d",
   "metadata": {
    "id": "2f69eb9d"
   },
   "outputs": [],
   "source": [
    "#testing for a single case\n",
    "print(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1953edd3",
   "metadata": {
    "id": "1953edd3"
   },
   "outputs": [],
   "source": [
    "#import library regex\n",
    "import re\n",
    "#remove all instances that aren't a-z or 0-9\n",
    "test_case = re.sub(\"[^a-zA-Z0-9\\s]\",'',test_case)\n",
    "print(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94643f7",
   "metadata": {
    "id": "f94643f7"
   },
   "outputs": [],
   "source": [
    "# Apply lowercasing to the entire text_dataset\n",
    "text_df['Article'] = text_df['Article'].str.lower()\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660b7966",
   "metadata": {
    "id": "660b7966"
   },
   "source": [
    "# Stop word Removal\n",
    "\n",
    "### Example english stopwords:\n",
    "#### the, is, and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a4d7d3",
   "metadata": {
    "id": "82a4d7d3"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.text_corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c8f66",
   "metadata": {
    "id": "4a0c8f66"
   },
   "outputs": [],
   "source": [
    "#download and import english stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d6b7f",
   "metadata": {},
   "source": [
    "### If there are a tagalog words found in the dataset, there is an available corpus for tagalog words\n",
    "### Example tagalog stopwords:\n",
    "#### at, pala, kaya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85d071",
   "metadata": {
    "id": "fd85d071"
   },
   "outputs": [],
   "source": [
    "#add tagalog stop words\n",
    "#import tagalog stop words file and convert it to list\n",
    "filename = 'tagalog_stop_words.txt'\n",
    "with open(filename, 'r') as file:\n",
    "    tagalog_words = file.read().splitlines()\n",
    "print(tagalog_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a48bf1",
   "metadata": {
    "id": "11a48bf1"
   },
   "outputs": [],
   "source": [
    "#add tagalog stop_words to english stopwords\n",
    "stop_words.extend(tagalog_words)\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9d65da",
   "metadata": {
    "id": "bb9d65da"
   },
   "source": [
    "### Removing all stopwords from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9398c",
   "metadata": {
    "id": "a1e9398c"
   },
   "outputs": [],
   "source": [
    "#testing it for a single case\n",
    "print(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9a53d6",
   "metadata": {
    "id": "0d9a53d6"
   },
   "outputs": [],
   "source": [
    "#split the text per space\n",
    "#print(test_case.split(' '))\n",
    "#initialize empty list for finalization of stopword removal\n",
    "final_test =[]\n",
    "#remove all empty and \\r\\n\n",
    "for word in test_case.split():\n",
    "    if word =='' or '\\r\\n' in word or word in stop_words:\n",
    "        None\n",
    "    else:\n",
    "        final_test.append(word)\n",
    "print(final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176a36ec",
   "metadata": {
    "id": "176a36ec"
   },
   "outputs": [],
   "source": [
    "#lets try to join the final test\n",
    "' '.join(final_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b2c23d",
   "metadata": {
    "id": "03b2c23d"
   },
   "source": [
    "## Applying to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5386c59",
   "metadata": {
    "id": "d5386c59"
   },
   "outputs": [],
   "source": [
    "#make a function pipeline:\n",
    "def remove_stopwords(article,stopwords):\n",
    "    #make a temporary list\n",
    "    temp_fnl = []\n",
    "    for word in article.split():\n",
    "        if word =='' or '\\r\\n' in word or word in stopwords:\n",
    "            None\n",
    "        else:\n",
    "            temp_fnl.append(word)\n",
    "    return ' '.join(temp_fnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9db1cf",
   "metadata": {
    "id": "8c9db1cf"
   },
   "outputs": [],
   "source": [
    "remove_stopwords(test_case,stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeedfc1",
   "metadata": {
    "id": "feeedfc1"
   },
   "outputs": [],
   "source": [
    "# Apply lowercasing to the entire text_dataset\n",
    "text_df['Article'] = text_df['Article'].str.lower()\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b1f506",
   "metadata": {
    "id": "68b1f506"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5338e022",
   "metadata": {
    "id": "5338e022"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Download the necessary text_data for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536c2782",
   "metadata": {
    "id": "536c2782"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc3444",
   "metadata": {
    "id": "11dc3444"
   },
   "outputs": [],
   "source": [
    "# Apply word_tokenize function to each article in the 'article' column\n",
    "text_df['tokenized_article'] = text_df['Article'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae24affa",
   "metadata": {
    "id": "ae24affa"
   },
   "outputs": [],
   "source": [
    "# Print the first 10 tokenized result\n",
    "text_df['tokenized_article'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c053a8",
   "metadata": {
    "id": "84c053a8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
